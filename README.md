## Steam Data Explorer

End-to-end pipeline: Steam API → Python → SQL DB → Power BI.

A robust data pipeline that extracts gaming data from Steam's Web API, processes it through Python ETL workflows, and stores it in SQL databases for analytics and visualization.

### Prerequisites

- Python 3.10 or higher
- Steam Web API key ([Get one here](https://steamcommunity.com/dev/apikey))
- Your Steam ID (64-bit format) - optional but recommended

### Quickstart

1) **Create and activate a Python environment:**
```bash
python -m venv venv
# Windows
venv\Scripts\activate
# macOS/Linux
source venv/bin/activate
```

2) **Install dependencies:**
```bash
pip install -r requirements.txt
```

3) **Configure environment:**
Create a `.env` file in the project root:
```env
STEAM_API_KEY=your_steam_api_key_here
DATABASE_URL=sqlite:///steam.db
STEAM_USER_ID64=your_steam_id_64_here
LOG_LEVEL=INFO
```

4) **Initialize database:**
```bash
python -m scripts.init_db
```

5) **Fetch and load sample data:**
```bash
# Fetch specific games (Counter-Strike 2, Dota 2) + your owned games
python -m scripts.fetch_and_load --apps 570,730 --owned --rps 2.0 --batch-size 50

# Or just fetch specific games without owned games
python -m scripts.fetch_and_load --apps 570,730

# Or just fetch your owned games
python -m scripts.fetch_and_load --owned
```

### Configuration & Logging
- Logging is centralized via `steam_explorer/logging_utils.py`. Control level via `LOG_LEVEL` env var (e.g., `DEBUG`, `INFO`).
- CLI flags:
  - `--rps`: requests per second rate limit (default 2.0).
  - `--batch-size`: chunk size for app details (default 50).

### Data Model (initial)
- `games` (dim): `appid` (PK), `name`, `type`, `is_free`, timestamps.
- `achievements_global` (fact): unique `(appid, name)`, `percent`, `created_at`.
- `ownerships` (fact): unique `(steamid, appid)`, `playtime_forever`, `created_at`.

### Validation & Quality Checks
- ETL validates data before insert:
  - Skips app rows without names, invalid IDs.
  - Achievement `percent` must be 0–100.
  - Normalizes/guards non-integer or negative playtime.
- Logs counts and skips for traceability.

### API Client Resilience
- Retries with exponential backoff for 429/5xx, basic rate limiting, and batched store API requests.

### Power BI Integration
Connect Power BI to your database:

**SQLite (Default):**
- Use "Get Data" → "Database" → "SQLite database"
- Point to your `steam.db` file

**PostgreSQL/MySQL:**
- Use "Get Data" → "Database" → "PostgreSQL/MySQL database"
- Use the same connection string from your `DATABASE_URL`
- Ensure proper ODBC drivers are installed

### Tests
Run the test suite:
```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=steam_explorer

# Run quietly
pytest -q
```
- Tests use fixtures from `tests/conftest.py`
- Database defaults to a temporary SQLite file
- No live network calls required; Steam API client is mocked
- All tests are isolated and can run without external dependencies

### Alembic (migrations)
- Configure `DATABASE_URL` in your environment (e.g., Postgres/MySQL) before running:
```bash
set DATABASE_URL=postgresql+psycopg://user:pass@host:5432/dbname   # Windows PowerShell: $env:DATABASE_URL="..."
```
- Upgrade to latest migration:
```bash
alembic upgrade head
```
- Create a new migration after changing models:
```bash
alembic revision -m "your change"
# edit the autogenerated file under alembic/versions/ if needed
alembic upgrade head
```
- Initial migration is provided at `alembic/versions/0001_initial.py` covering `games`, `achievements_global`, `ownerships`.

### Must-have vs Optional
- Must-have:
  - Working ETL scripts (`init_db.py`, `fetch_and_load.py`).
  - Config via env, logging, validation, retries/rate limits, batching.
  - Clear README with run instructions.
  - Tests skeleton with fixtures and basic coverage.
  - Alembic config and initial migration.
- Optional (portfolio-plus):
  - Additional endpoints and models.
  - Docker/compose for Postgres, CI (pytest + ruff/black), data docs.

### Project Structure
```
steam_explorer/
├── api/                    # Steam API client
│   └── steam_client.py
├── etl/                    # ETL pipeline logic
│   └── pipeline.py
├── models.py              # SQLAlchemy data models
├── db.py                  # Database connection utilities
├── config.py              # Configuration management
└── logging_utils.py       # Centralized logging

scripts/
├── init_db.py            # Database initialization
└── fetch_and_load.py     # Main ETL script

alembic/                  # Database migrations
tests/                    # Test suite
```

### Extending the Pipeline
1. **Add new Steam API endpoints:**
   - Extend `steam_explorer/api/steam_client.py`
   - Add new methods following existing patterns

2. **Create new data models:**
   - Add models to `steam_explorer/models.py`
   - Generate migration: `alembic revision -m "add new model"`
   - Apply migration: `alembic upgrade head`

3. **Add data transformations:**
   - Create transform functions in `steam_explorer/etl/pipeline.py`
   - Wire into `scripts/fetch_and_load.py`

### Troubleshooting

**Common Issues:**
- **API Rate Limits:** Reduce `--rps` value (default 2.0)
- **Database Locked:** Ensure no other processes are using the database
- **Missing API Key:** Verify `STEAM_API_KEY` is set in `.env`
- **Invalid Steam ID:** Use 64-bit Steam ID format

**Getting Your Steam ID:**
1. Visit [SteamID.io](https://steamid.io/)
2. Enter your Steam profile URL
3. Copy the "steamID64" value

