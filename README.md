## Steam Data Explorer

End-to-end pipeline: Steam API → Python → SQL DB → Power BI.

A robust data pipeline that extracts gaming data from Steam's Web API, processes it through Python ETL workflows, and stores it in SQL databases for analytics and visualization.

### Prerequisites

- Python 3.10 or higher
- Steam Web API key ([Get one here](https://steamcommunity.com/dev/apikey))
- Your Steam ID (64-bit format) - optional but recommended

### Quickstart

1) **Create and activate a Python environment:**
```bash
python -m venv venv
# Windows
venv\Scripts\activate
# macOS/Linux
source venv/bin/activate
```

2) **Install dependencies:**
```bash
pip install -r requirements.txt
```

3) **Configure environment:**
Create a `.env` file in the project root:
```env
STEAM_API_KEY=your_steam_api_key_here
DATABASE_URL=sqlite:///steam.db
STEAM_USER_ID64=your_steam_id_64_here
LOG_LEVEL=INFO
```

4) **Run the Steam Manager:**
```bash
python steam_manager.py
```

That's it! The Steam Manager provides an interactive menu to:
- Initialize your database
- Fetch your Steam games and data
- Explore your gaming statistics
- Manage your data pipeline

### Using the Steam Manager

The `steam_manager.py` provides a user-friendly interface with these options:

**📊 DATA MANAGEMENT**
- Initialize Database
- Fetch Game Details (by App IDs)
- Fetch Your Owned Games
- Fetch All Missing Game Details

**🔍 DATA EXPLORATION**
- Database Explorer (Interactive)
- View Data Summary
- Find Game IDs by Name

**🛠️ UTILITIES**
- Test Database Connection
- Create Database Views
- Add Game Names to Ownerships Table

**📖 HELP & INFO**
- Show Project Structure
- Show Configuration

### Configuration & Logging
- Logging is centralized via `steam_explorer/logging_utils.py`. Control level via `LOG_LEVEL` env var (e.g., `DEBUG`, `INFO`).
- CLI flags:
  - `--rps`: requests per second rate limit (default 2.0).
  - `--batch-size`: chunk size for app details (default 50).

### Data Model
- `games` (dim): `appid` (PK), `name`, `type`, `is_free`, timestamps.
- `achievements_global` (fact): unique `(appid, name)`, `percent`, `created_at`.
- `ownerships` (fact): unique `(steamid, appid)`, `game_name`, `playtime_forever`, `created_at`.

**Enhanced Features:**
- Ownership records now include `game_name` for easier querying and Power BI integration
- Automatic game name population during data fetching
- MySQL compatibility improvements

### Validation & Quality Checks
- ETL validates data before insert:
  - Skips app rows without names, invalid IDs.
  - Achievement `percent` must be 0–100.
  - Normalizes/guards non-integer or negative playtime.
- Logs counts and skips for traceability.

### API Client Resilience
- Retries with exponential backoff for 429/5xx, basic rate limiting, and batched store API requests.

### Power BI Integration
Connect Power BI to your database:

**SQLite (Default):**
- Use "Get Data" → "Database" → "SQLite database"
- Point to your `steam.db` file

**PostgreSQL/MySQL:**
- Use "Get Data" → "Database" → "PostgreSQL/MySQL database"
- Use the same connection string from your `DATABASE_URL`
- Ensure proper ODBC drivers are installed

### Tests
Run the test suite:
```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=steam_explorer

# Run quietly
pytest -q
```
- Tests use fixtures from `tests/conftest.py`
- Database defaults to a temporary SQLite file
- No live network calls required; Steam API client is mocked
- All tests are isolated and can run without external dependencies

### Alembic (migrations)
- Configure `DATABASE_URL` in your environment (e.g., Postgres/MySQL) before running:
```bash
set DATABASE_URL=postgresql+psycopg://user:pass@host:5432/dbname   # Windows PowerShell: $env:DATABASE_URL="..."
```
- Upgrade to latest migration:
```bash
alembic upgrade head
```
- Create a new migration after changing models:
```bash
alembic revision -m "your change"
# edit the autogenerated file under alembic/versions/ if needed
alembic upgrade head
```
- Initial migration is provided at `alembic/versions/0001_initial.py` covering `games`, `achievements_global`, `ownerships`.

### Key Features

**✅ Complete Data Pipeline:**
- Steam API integration with rate limiting and error handling
- ETL workflows with data validation and quality checks
- Support for SQLite, MySQL, and PostgreSQL databases
- Automatic game name resolution and data enrichment

**✅ User-Friendly Interface:**
- Interactive menu system via `steam_manager.py`
- Beautiful data summaries with gaming insights
- Database explorer for detailed data browsing
- Command-line tools for advanced users

**✅ Production Ready:**
- Comprehensive error handling and logging
- Database migrations with Alembic
- Test suite with mocked API calls
- Configurable via environment variables

**✅ Analytics Ready:**
- Power BI integration support
- Optimized database schema for reporting
- Game ownership and playtime tracking
- Achievement completion statistics

### Project Structure
```
steam_data_explorer/
├── steam_manager.py          # 🎮 Main interface - run this!
├── .env                      # Configuration file
├── requirements.txt          # Python dependencies
├── README.md                 # Documentation
│
├── steam_explorer/           # Core application code
│   ├── __init__.py
│   ├── config.py            # Configuration management
│   ├── db.py                # Database connections
│   ├── models.py            # Data models (with game_name support)
│   ├── logging_utils.py     # Logging utilities
│   ├── api/
│   │   └── steam_client.py  # Steam API client
│   └── etl/
│       └── pipeline.py      # Data transformation logic
│
├── tools/                   # Utility scripts (organized)
│   ├── init_db.py          # Initialize database
│   ├── fetch_games.py      # Fetch game data
│   ├── database_explorer.py # Interactive data browser
│   ├── find_game_ids.py    # Find Steam game IDs
│   ├── view_data.py        # Data summary with insights
│   ├── test_connection.py  # Database connection test
│   ├── fetch_all_owned_games.py # Fetch all missing games
│   └── ... (other utilities)
│
├── scripts/                 # Legacy scripts (for compatibility)
├── alembic/                 # Database migrations
└── tests/                   # Test suite
```

### Advanced Usage

**Command Line Interface (Alternative):**
If you prefer command-line usage over the interactive menu:
```bash
# Initialize database
python tools/init_db.py

# Fetch specific games
python tools/fetch_games.py --apps 570,730 --batch-size 1

# Fetch your owned games
python tools/fetch_games.py --owned

# View data summary
python tools/view_data.py

# Interactive database explorer
python tools/database_explorer.py
```

### Extending the Pipeline
1. **Add new Steam API endpoints:**
   - Extend `steam_explorer/api/steam_client.py`
   - Add new methods following existing patterns

2. **Create new data models:**
   - Add models to `steam_explorer/models.py`
   - Generate migration: `alembic revision -m "add new model"`
   - Apply migration: `alembic upgrade head`

3. **Add data transformations:**
   - Create transform functions in `steam_explorer/etl/pipeline.py`
   - Wire into `tools/fetch_games.py`

4. **Add new utilities:**
   - Create new scripts in `tools/` directory
   - Add menu options to `steam_manager.py` if needed

### Troubleshooting

**Common Issues:**
- **API Rate Limits:** Use the Steam Manager's built-in rate limiting or reduce `--rps` value
- **Database Connection:** Use option 8 in Steam Manager to test your database connection
- **Missing API Key:** Verify `STEAM_API_KEY` is set in `.env` (use option 12 to check configuration)
- **Invalid Steam ID:** Use 64-bit Steam ID format
- **Unknown Games:** Use option 4 in Steam Manager to fetch missing game details
- **MySQL Syntax Errors:** The project now includes MySQL compatibility fixes

**Getting Your Steam ID:**
1. Visit [SteamID.io](https://steamid.io/)
2. Enter your Steam profile URL
3. Copy the "steamID64" value

**Getting Help:**
- Run `python steam_manager.py` and use option 11 for project structure
- Use option 12 to view your current configuration
- Check the interactive database explorer (option 5) for data insights

